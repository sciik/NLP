{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd7774ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c52e6ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(\"./data/ko_32000.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aaf47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_mask(tokens, vacab_list):\n",
    "    tokens_orgin = copy.deepcopy(tokens)\n",
    "    word_index = []\n",
    "    temp = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token != \"[CLS]\" and token != \"[SEP]\":\n",
    "            if i > 1 and token.startswith(u\"\\u2581\"):\n",
    "                word_index.append(temp)\n",
    "                temp = [i]\n",
    "            else:\n",
    "                temp.append(i)\n",
    "                \n",
    "    max_count = int((len(word_index)-3)*0.15)\n",
    "    mask_list = []\n",
    "    while len(mask_list) < max_count:\n",
    "        index = random.choice(word_index)\n",
    "        if index not in mask_list: \n",
    "            mask_list.append(index)\n",
    "        \n",
    "            per = random.randint(0, 100)\n",
    "        \n",
    "            if per < 80:\n",
    "                for i in index: tokens[i] = \"[MASK]\"\n",
    "            elif per > 90:\n",
    "                for i in index: tokens[i] = random.choice(vacab_list)\n",
    "                    \n",
    "    mask_index = sorted([j for i in mask_list for j in i])\n",
    "    mask_label = [tokens_orgin[i] for i in mask_index]\n",
    "    \n",
    "    return tokens, mask_index, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7189b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tokens(left_tokens, right_tokens, max_len):\n",
    "    total_len = len(left_tokens) + len(right_tokens)\n",
    "    while total_len > max_len:\n",
    "        if len(left_tokens) > len(right_tokens): del left_tokens[0]\n",
    "        else: right_tokens.pop()\n",
    "        total_len -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fb698f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(vocab, doc, n_seq, vocab_list):\n",
    "    max_len = n_seq-3\n",
    "    \n",
    "    instances = []\n",
    "    temp_sentences = []\n",
    "    temp_len = 0\n",
    "    for i, sentence in enumerate(doc):\n",
    "        temp_sentences.append(sentence)\n",
    "        temp_len += len(sentence)\n",
    "        \n",
    "        if len(temp_sentences) > 1 and (i == len(doc)-1 or temp_len >= max_len):\n",
    "            pivot = random.randrange(1, len(temp_sentences))\n",
    "            \n",
    "            left_tokens = []\n",
    "            right_tokens = []\n",
    "            for j in range(pivot): left_tokens += temp_sentences[j]\n",
    "            for j in range(pivot, len(temp_sentences)): right_tokens += temp_sentences[j]\n",
    "            \n",
    "            is_next = 0\n",
    "            if random.randint(0, 100) < 50:\n",
    "                tokens_temp = left_tokens\n",
    "                left_tokens = right_tokens\n",
    "                right_tokens = tokens_temp\n",
    "            else:\n",
    "                is_next = 1\n",
    "                \n",
    "            trim_tokens(left_tokens, right_tokens, max_len)\n",
    "            assert 0 < len(left_tokens), \"zero left token\"\n",
    "            assert 0 < len(right_tokens), \"zero right token\"\n",
    "            \n",
    "            tokens = [\"[CLS]\"] + left_tokens + [\"[SEP]\"] + right_tokens + [\"[SEP]\"]\n",
    "            segment = [0] * (len(left_tokens) + 2) + [1] * (len(right_tokens) + 1)\n",
    "            \n",
    "            tokens, mask_index, mask_label = create_pretrain_mask(tokens, vocab_list)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_index,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "            \n",
    "            temp_sentences = []\n",
    "            temp_len = 0\n",
    "            \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67c4dacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    def save_pretrain_instances(out_f, doc, vocab_list):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    vocab_list = []\n",
    "    for id in range(7, 8000):\n",
    "        if not vocab.is_unknown(id):      \n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  \n",
    "                    if 0 < len(doc):\n",
    "                        save_pretrain_instances(out_f, doc, vocab_list)\n",
    "                        doc = []\n",
    "                else: \n",
    "                    pieces = vocab.encode_as_pieces(line)\n",
    "                    if 0 < len(pieces):\n",
    "                        doc.append(pieces)\n",
    "            if 0 < len(doc): \n",
    "                save_pretrain_instances(out_f, doc)\n",
    "                doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbf96b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1682c2d405b47ff89c106f6aadb09b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrain_json_path = \"./data/bert_pre_train.json\"\n",
    "corpus_file = \"./data/kowiki.txt\"\n",
    "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcf9b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            \n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            \n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            \n",
    "            label_nsp = data[\"is_next\"]\n",
    "            \n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7553062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11b187637fd4007a60808a62790aaa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32/682462427.py:29: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
      "/tmp/ipykernel_32/682462427.py:30: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
      "/tmp/ipykernel_32/682462427.py:31: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 128000 128000\n"
     ]
    }
   ],
   "source": [
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "896c27f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "febc94ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d44b91c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_initializer(stddev=0.02):\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "def bias_initializer():\n",
    "    return tf.zeros_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec418460",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf95bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30da2fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6074d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4427fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        \n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        \n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        \n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)#(bs, q_len, n_head, d_head)\n",
    "                                                        #(bs, n_head, len, len)\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "                                                                # (bs, num_head, d_model, Q_len)\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2d0865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e81a2a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e916f8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3299d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d15a437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cdf816d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 256,\n",
       " 'n_head': 4,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 1024,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 3,\n",
       " 'n_seq': 256,\n",
       " 'n_vocab': 32007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\"d_model\": 256, \"n_head\": 4, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 3, \"n_seq\": 256, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "287911bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seq = 10\n",
    "\n",
    "enc_tokens = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "segments = np.random.randint(0, 2, (10, n_seq))\n",
    "labels_nsp = np.random.randint(0, 2, (10,))\n",
    "labels_mlm = np.random.randint(0, len(vocab), (10, n_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ece5e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    \n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22d780cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    \n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    \n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fecc93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d3235d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segments (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 256), (None, 10629632    enc_tokens[0][0]                 \n",
      "                                                                 segments[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            66304       bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 32007)  0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 10,695,936\n",
      "Trainable params: 10,695,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d657786",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d85bfbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 567s 279ms/step - loss: 20.5859 - nsp_loss: 0.6476 - mlm_loss: 19.9382 - nsp_acc: 0.5992 - mlm_lm_acc: 0.0936\n",
      "\n",
      "Epoch 00001: mlm_lm_acc improved from -inf to 0.09363, saving model to ./data/bert_pre_train.hdf5\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 566s 283ms/step - loss: 18.4141 - nsp_loss: 0.6133 - mlm_loss: 17.8008 - nsp_acc: 0.6459 - mlm_lm_acc: 0.1389\n",
      "\n",
      "Epoch 00002: mlm_lm_acc improved from 0.09363 to 0.13888, saving model to ./data/bert_pre_train.hdf5\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 567s 283ms/step - loss: 16.8231 - nsp_loss: 0.5866 - mlm_loss: 16.2366 - nsp_acc: 0.6931 - mlm_lm_acc: 0.1563\n",
      "\n",
      "Epoch 00003: mlm_lm_acc improved from 0.13888 to 0.15634, saving model to ./data/bert_pre_train.hdf5\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 567s 283ms/step - loss: 15.3052 - nsp_loss: 0.5570 - mlm_loss: 14.7482 - nsp_acc: 0.7333 - mlm_lm_acc: 0.1829\n",
      "\n",
      "Epoch 00004: mlm_lm_acc improved from 0.15634 to 0.18289, saving model to ./data/bert_pre_train.hdf5\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 567s 284ms/step - loss: 14.3693 - nsp_loss: 0.5235 - mlm_loss: 13.8458 - nsp_acc: 0.7758 - mlm_lm_acc: 0.2029\n",
      "\n",
      "Epoch 00005: mlm_lm_acc improved from 0.18289 to 0.20286, saving model to ./data/bert_pre_train.hdf5\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 567s 284ms/step - loss: 13.7320 - nsp_loss: 0.4906 - mlm_loss: 13.2414 - nsp_acc: 0.8142 - mlm_lm_acc: 0.2163\n",
      "\n",
      "Epoch 00006: mlm_lm_acc improved from 0.20286 to 0.21628, saving model to ./data/bert_pre_train.hdf5\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 568s 284ms/step - loss: 13.2796 - nsp_loss: 0.4647 - mlm_loss: 12.8149 - nsp_acc: 0.8434 - mlm_lm_acc: 0.2262\n",
      "\n",
      "Epoch 00007: mlm_lm_acc improved from 0.21628 to 0.22620, saving model to ./data/bert_pre_train.hdf5\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 568s 284ms/step - loss: 12.9712 - nsp_loss: 0.4445 - mlm_loss: 12.5267 - nsp_acc: 0.8652 - mlm_lm_acc: 0.2328\n",
      "\n",
      "Epoch 00008: mlm_lm_acc improved from 0.22620 to 0.23277, saving model to ./data/bert_pre_train.hdf5\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 568s 284ms/step - loss: 12.7832 - nsp_loss: 0.4317 - mlm_loss: 12.3514 - nsp_acc: 0.8790 - mlm_lm_acc: 0.2370\n",
      "\n",
      "Epoch 00009: mlm_lm_acc improved from 0.23277 to 0.23701, saving model to ./data/bert_pre_train.hdf5\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 568s 284ms/step - loss: 12.6951 - nsp_loss: 0.4257 - mlm_loss: 12.2694 - nsp_acc: 0.8857 - mlm_lm_acc: 0.2387\n",
      "\n",
      "Epoch 00010: mlm_lm_acc improved from 0.23701 to 0.23868, saving model to ./data/bert_pre_train.hdf5\n"
     ]
    }
   ],
   "source": [
    "save_weights = tf.keras.callbacks.ModelCheckpoint(\"./data/bert_pre_train.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
    "\n",
    "history = pre_train_model.fit(pre_train_inputs, pre_train_labels, epochs=epochs, batch_size=batch_size, callbacks=[save_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa07f2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9e8c19",
   "metadata": {},
   "source": [
    "## 결론\n",
    "### BERT를 구현하고 한국어 위키피디아 데이터로 pretrain시켜봤습니다. 역시 개발자 출신이 아니여서그런지 json쓰는 거는 여전히 까다롭네ㅎㅎ 그나저나 노드 만드신분 짬 좀 되시던데 코드는 개판ㅠㅠㅠ 역시 짬과 코드는 비례하지 않는다는 명제 다시 확인\n",
    "### 그나마 쫌 큰 데이터를 처음 다뤄보는데 역시 많은 난관들이 보입니다. 메모리맵핑쓰는거나 컨피규 커널 세팅등 여러부분에서 디테일하게 잡을텐데 나같은 코더는 너무 귀찮은~~~~~개발자는 못해먹겟닼ㅋㅋㅋㅋ\n",
    "### 어쨋든 간만에 코딩다운 코딩좀 해보고 나름 재미났던 프로젝트 사실상 사전학습만시키고 써보진않아서 성능평가가 안됫던 프로젝트 이제부터 써보면 되지 머~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe943f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
