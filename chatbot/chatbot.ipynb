{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c52e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from konlpy.tag import Mecab\n",
    "import nltk\n",
    "import gensim\n",
    "import random\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "687faefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./ChatbotData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83fe07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data\n",
    "question = data[\"Q\"]\n",
    "answer = data[\"A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68ecaffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentences):\n",
    "    s = []\n",
    "    for sentence in sentences:\n",
    "        temp = sentence.lower().strip()\n",
    "        temp = re.sub(\"ㅋ+\", \" ㅋㅋ\", temp)\n",
    "        temp = re.sub(\"ㅎ+\", \" ㅎㅎ\", temp)\n",
    "        temp = re.sub(\"ㅜ+\", \" ㅜㅜ\", temp)\n",
    "        temp = re.sub(\"ㅠ+\", \" ㅠㅠ\", temp)\n",
    "        temp = re.sub(r\"!+\", \"!\", temp)\n",
    "        temp = re.sub(r\"~+\", \"~\", temp)\n",
    "        temp = re.sub(r\";+\", \";\", temp)\n",
    "        temp = re.sub(r\"\\\"\", \"\", temp)\n",
    "        temp = re.sub(r\"([.,!?~;])\", r\" \\1\", temp)\n",
    "        s.append(temp)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f02503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = preprocessing(question)\n",
    "answer = preprocessing(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "428b9894",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 10\n",
    "question = question[valid_size:]\n",
    "answer = answer[valid_size:]\n",
    "\n",
    "valid_input = question[:valid_size]\n",
    "valid_output = answer[:valid_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4edf0248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sns보면 나만 빼고 다 행복해보여', '가끔 궁금해', '가끔 뭐하는지 궁금해', '가끔은 혼자인게 좋다', '가난한 자의 설움', '가만 있어도 땀난다', '가상화폐 쫄딱 망함', '가스불 켜고 나갔어', '가스불 켜놓고 나온거 같아', '가스비 너무 많이 나왔다 .']\n"
     ]
    }
   ],
   "source": [
    "print(question[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94581288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_length(question, answer):\n",
    "    temp_question = []\n",
    "    temp_answer = []\n",
    "    for q, a in zip(question, answer):\n",
    "        if len(q)< 40 and len(a) < 40:\n",
    "            temp_question.append(q)\n",
    "            temp_answer.append(a)\n",
    "    return temp_question, temp_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ddbac52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11678 11678\n"
     ]
    }
   ],
   "source": [
    "question, answer = cut_length(question, answer)\n",
    "print(len(question), len(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78aa47c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "question, test_input, answer, test_output = train_test_split(question, answer, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "474cf10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(source, target, tokenizer):\n",
    "    words = []\n",
    "    source_tokens = []\n",
    "    for s in source:\n",
    "        temp = tokenizer.morphs(s)\n",
    "        source_tokens.append(temp)\n",
    "        words += temp\n",
    "    \n",
    "    target_tokens = []\n",
    "    for t in target:\n",
    "        temp = tokenizer.morphs(t)\n",
    "        target_tokens.append(temp)\n",
    "        words += temp\n",
    "        \n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common()\n",
    "    \n",
    "    words = [\"<pad>\", \"<sos>\", \"<eos>\"] + [key for key, _ in counter]\n",
    "    word_to_index = {word : index for index, word in enumerate(words)}\n",
    "    \n",
    "    source_corpus = []\n",
    "    target_corpus = []\n",
    "    for source_token in source_tokens:\n",
    "        source_corpus.append([word_to_index[s] for s in source_token])\n",
    "    for target_token in target_tokens:\n",
    "        target_corpus.append([word_to_index[t] for t in target_token])\n",
    "        \n",
    "    return source_corpus, target_corpus, word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a576863",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()\n",
    "que_corpus, ans_corpus, word_to_index = build_corpus(question, answer, mecab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c47a4853",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:value for index, value in enumerate(word_to_index)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af8fd357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentence(tokens):\n",
    "    return [index_to_word[token] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "324b551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_tokens(sentence):\n",
    "    return [word_to_index[s] for s in sentence.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a94845b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "file_path = \"./ko.bin\"\n",
    "wv_model = Word2Vec.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2fc5d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub(toks, word2vec):\n",
    "    res = \"\"\n",
    "    \n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "        \n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res += _to + \" \"\n",
    "        else: res += tok + \" \"\n",
    "\n",
    "    return res, _to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "daaf056e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_435/2220321154.py:6: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  _to = word2vec.most_similar(_from)[0][0]\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "for q, a in zip(que_corpus, ans_corpus):\n",
    "    questions.append(q)\n",
    "    answers.append(a)\n",
    "    \n",
    "    temp, new_word = lexical_sub(make_sentence(q), wv_model)\n",
    "    if temp is not None and new_word in word_to_index:\n",
    "        questions.append(return_tokens(temp))\n",
    "        answers.append(a)\n",
    "    \n",
    "    temp, new_word = lexical_sub(make_sentence(a), wv_model)\n",
    "    if temp is not None and new_word in word_to_index:\n",
    "        questions.append(q)\n",
    "        answers.append(return_tokens(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88873d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [[word_to_index[\"<sos>\"]] + corpus + [word_to_index[\"<eos>\"]] for corpus in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b56d40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21105 21105\n",
      "[[3108, 5, 6, 43, 4, 62, 2170, 107, 111, 3, 529, 125, 663, 193, 29, 65, 58, 102], [3108, 5, 6, 43, 4, 62, 2170, 107, 111, 3, 529, 125, 663, 193, 29, 65, 58, 102], [494, 4366, 100, 107, 67, 3], [494, 4366, 100, 107, 67, 3], [3109, 466, 6, 168, 94, 949, 104, 24, 18, 19], [2568, 130, 14], [2568, 130, 14], [65, 582, 59, 334], [105, 6, 194, 403, 21, 682, 184], [2127, 6, 194, 403, 21, 682, 184]]\n",
      "[[1, 2170, 7, 1235, 15, 8, 3, 2], [1, 2170, 7, 1235, 15, 8, 50, 2], [1, 113, 57, 48, 5, 6, 26, 17, 2301, 5, 8, 3, 2], [1, 113, 57, 48, 1120, 6, 26, 17, 2301, 1120, 8, 3, 2], [1, 2977, 100, 107, 168, 4, 147, 104, 7, 13, 27, 3, 2], [1, 47, 365, 40, 16, 7, 13, 27, 3, 2], [1, 47, 365, 40, 16, 30, 13, 27, 3, 2], [1, 179, 1054, 4, 38, 20, 24, 28, 3, 2], [1, 565, 32, 166, 115, 45, 286, 59, 13, 147, 257, 28, 3, 2], [1, 565, 32, 166, 115, 45, 286, 59, 13, 147, 257, 28, 3, 2]]\n",
      "6515\n"
     ]
    }
   ],
   "source": [
    "print(len(questions), len(answers))\n",
    "print(questions[:10])\n",
    "print(answers[:10])\n",
    "print(len(word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e46e0a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = tf.keras.preprocessing.sequence.pad_sequences(questions, padding='post')\n",
    "decoder_input = tf.keras.preprocessing.sequence.pad_sequences(answers, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc7be8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((encoder_input, decoder_input)).batch(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19b25c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    table = np.zeros((pos, d_model))\n",
    "    for i in range(pos):\n",
    "        for j in range(d_model):\n",
    "            table[i][j] = i / (10000 ** (2*(j//2) / d_model))\n",
    "    \n",
    "    sinusoid_table = table\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])   \n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a172514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_lookahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d0b3fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5b3fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4290165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        \n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e0806e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9a8066fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask): \n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "549840a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "507ab63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "49d1e8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=4,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=len(word_to_index),\n",
    "    tgt_vocab_size=len(word_to_index),\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0efec3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "693a284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(512)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d2c484fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8d95d9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1] \n",
    "    gold = tgt[:, 1:]   \n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58f0db39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84fada199f94c1396376ed994e3aa61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "    tqdm_bar = tqdm(total=dataset_count)\n",
    "    for step, (enc_batch, dec_batch) in enumerate(train_dataset):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_batch,\n",
    "                    dec_batch,\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        tqdm_bar.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        tqdm_bar.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (step + 1)))\n",
    "        tqdm_bar.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f76532b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_sentence):\n",
    "    sentence = preprocessing([input_sentence])\n",
    "    input_sentence = [word_to_index[s] for s in mecab.morphs(input_sentence)]\n",
    "    input_sentence = tf.keras.preprocessing.sequence.pad_sequences([input_sentence], padding=\"post\", maxlen=39)\n",
    "    \n",
    "    output_sentence = []\n",
    "    output_sequence = tf.expand_dims([word_to_index[\"<sos>\"]], 0)\n",
    "    \n",
    "    flag = True\n",
    "    while(flag):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(input_sentence, output_sequence)\n",
    "        predictions, _, _, _ = transformer(input_sentence, output_sequence, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "        \n",
    "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "        \n",
    "        if predicted_id == word_to_index[\"<eos>\"]:\n",
    "            flag = False\n",
    "            \n",
    "        output_sentence.append(index_to_word[predicted_id])\n",
    "        output_sequence = tf.concat([output_sequence, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "    return \" \".join(output_sentence[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e1810dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "챙겨 챙겨 챙겨 챙겨 드세요 .\n",
      "맛있 게 드세요 !\n",
      "맛있 게 드세요 .\n"
     ]
    }
   ],
   "source": [
    "print(translate(\"배고파\"))\n",
    "print(translate(\"밥먹을까\"))\n",
    "print(translate(\"김치찌개 먹을래\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b7eed6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu_single(model, src_sentence, tgt_sentence, verbose=True):\n",
    "    if (len(src_sentence) > 40): return None\n",
    "    if (len(tgt_sentence) > 40): return None\n",
    "\n",
    "    reference = tgt_sentence\n",
    "    candidate = translate(src_sentence)\n",
    "\n",
    "    score = sentence_bleu([reference], candidate,\n",
    "                          smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Source Sentence: \", src_sentence)\n",
    "        print(\"Model Prediction: \", candidate)\n",
    "        print(\"Real: \", reference)\n",
    "        print(\"Score: %lf\\n\" % score)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e44911c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(model, src_sentences, tgt_sentence, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(src_sentences)\n",
    "    \n",
    "    for idx in tqdm(range(sample_size)):\n",
    "        score = eval_bleu_single(model, src_sentences[idx], tgt_sentence[idx], verbose)\n",
    "        if not score: continue\n",
    "        \n",
    "        total_score += score\n",
    "    \n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "18af055b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e00e88f59b4861b5a55a74aad8cc88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  sns보면 나만 빼고 다 행복해보여\n",
      "Model Prediction:  자랑 하 는 자리 니까요 .\n",
      "Real:  자랑하는 자리니까요 .\n",
      "Score: 0.457883\n",
      "\n",
      "Source Sentence:  가끔 궁금해\n",
      "Model Prediction:  그럴 수 있 어요 .\n",
      "Real:  그 사람도 그럴 거예요 .\n",
      "Score: 0.128357\n",
      "\n",
      "Source Sentence:  가끔 뭐하는지 궁금해\n",
      "Model Prediction:  애정 과 표현 하 는 게 좋 을 것 같 아요 .\n",
      "Real:  그 사람도 그럴 거예요 .\n",
      "Score: 0.042764\n",
      "\n",
      "Source Sentence:  가끔은 혼자인게 좋다\n",
      "Model Prediction:  혼자 가 서 도 중요 해요 .\n",
      "Real:  혼자를 즐기세요 .\n",
      "Score: 0.080121\n",
      "\n",
      "Source Sentence:  가난한 자의 설움\n",
      "Model Prediction:  다시 다시 다시 다시 들어올 거 예요 .\n",
      "Real:  돈은 다시 들어올 거예요 .\n",
      "Score: 0.475952\n",
      "\n",
      "Source Sentence:  가만 있어도 땀난다\n",
      "Model Prediction:  땀 식혀 주 세요 .\n",
      "Real:  땀을 식혀주세요 .\n",
      "Score: 0.361328\n",
      "\n",
      "Source Sentence:  가상화폐 쫄딱 망함\n",
      "Model Prediction:  좋 은 사람 이 길 바라 요 .\n",
      "Real:  어서 잊고 새출발 하세요 .\n",
      "Score: 0.067701\n",
      "\n",
      "Source Sentence:  가스불 켜고 나갔어\n",
      "Model Prediction:  천천히 집 에 돌아가 돌아가 돌아가 고 있 어요 .\n",
      "Real:  빨리 집에 돌아가서 끄고 나오세요 .\n",
      "Score: 0.202849\n",
      "\n",
      "Source Sentence:  가스불 켜놓고 나온거 같아\n",
      "Model Prediction:  빨리 집 에 돌아가 돌아가 돌아가 돌아가 고 있 가 세요 .\n",
      "Real:  빨리 집에 돌아가서 끄고 나오세요 .\n",
      "Score: 0.266153\n",
      "\n",
      "Source Sentence:  가스비 너무 많이 나왔다 .\n",
      "Model Prediction:  마음 이 복잡 하 길 바랄게요 .\n",
      "Real:  다음 달에는 더 절약해봐요 .\n",
      "Score: 0.073126\n",
      "\n",
      "Num of Sample: 10\n",
      "Total Score: 0.21562346115427547\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(transformer, valid_input, valid_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
