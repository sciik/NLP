{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d820f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import konlpy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45cf638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_train_data = []\n",
    "english_train_data = []\n",
    "\n",
    "with open(\"./data/korean-english-park.train.ko\", \"r\") as f:\n",
    "    korean_train_data = f.read().splitlines()\n",
    "\n",
    "with open(\"./data/korean-english-park.train.en\", \"r\") as f:\n",
    "    english_train_data = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a9614cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>korean</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"</td>\n",
       "      <td>Much of personal computing is about \"can you t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>모든 광마우스와 마찬가지 로 이 광마우스도 책상 위에 놓는 마우스 패드를 필요로 하...</td>\n",
       "      <td>so a mention a few weeks ago about a rechargea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그러나 이것은 또한 책상도 필요로 하지 않는다.</td>\n",
       "      <td>Like all optical mice, But it also doesn't nee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79.95달러하는 이 최첨단 무선 광마우스는 허공에서 팔목, 팔, 그외에 어떤 부분...</td>\n",
       "      <td>uses gyroscopic sensors to control the cursor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>정보 관리들은 동남 아시아에서의 선박들에 대한 많은 (테러) 계획들이 실패로 돌아갔...</td>\n",
       "      <td>Intelligence officials have revealed a spate o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94118</th>\n",
       "      <td>“우리는 3월 8일 김승연 회장과 그의 아들이 보복폭행에 가담한 혐의를 찾기 위해 ...</td>\n",
       "      <td>””We are hoping to seize material evidence to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94119</th>\n",
       "      <td>월요일 술집 종업원 6명은 김회장과 아들에게 폭행을 당했음을 진술했다고 경찰은 말했다.</td>\n",
       "      <td>” On Monday, police secured statements from si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94120</th>\n",
       "      <td>그러나 불충분한 증거 확보로 수사에 어려움이 있다.</td>\n",
       "      <td>But the lack of material evidence is making it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94121</th>\n",
       "      <td>김회장과 그의 아들은 보복폭행 혐의를 강력히 부인하고 있다.</td>\n",
       "      <td>Kim and his son both deny the allegations.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94122</th>\n",
       "      <td>경찰은 김회장의 집무실에서 추가 증거를 찾은 이후 가능한 한 오늘 김회장과 아들을 ...</td>\n",
       "      <td>Police are planning to seek arrest warrants fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94123 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  korean  \\\n",
       "0                   개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"   \n",
       "1      모든 광마우스와 마찬가지 로 이 광마우스도 책상 위에 놓는 마우스 패드를 필요로 하...   \n",
       "2                             그러나 이것은 또한 책상도 필요로 하지 않는다.   \n",
       "3      79.95달러하는 이 최첨단 무선 광마우스는 허공에서 팔목, 팔, 그외에 어떤 부분...   \n",
       "4      정보 관리들은 동남 아시아에서의 선박들에 대한 많은 (테러) 계획들이 실패로 돌아갔...   \n",
       "...                                                  ...   \n",
       "94118  “우리는 3월 8일 김승연 회장과 그의 아들이 보복폭행에 가담한 혐의를 찾기 위해 ...   \n",
       "94119   월요일 술집 종업원 6명은 김회장과 아들에게 폭행을 당했음을 진술했다고 경찰은 말했다.   \n",
       "94120                       그러나 불충분한 증거 확보로 수사에 어려움이 있다.   \n",
       "94121                  김회장과 그의 아들은 보복폭행 혐의를 강력히 부인하고 있다.   \n",
       "94122  경찰은 김회장의 집무실에서 추가 증거를 찾은 이후 가능한 한 오늘 김회장과 아들을 ...   \n",
       "\n",
       "                                                 english  \n",
       "0      Much of personal computing is about \"can you t...  \n",
       "1      so a mention a few weeks ago about a rechargea...  \n",
       "2      Like all optical mice, But it also doesn't nee...  \n",
       "3      uses gyroscopic sensors to control the cursor ...  \n",
       "4      Intelligence officials have revealed a spate o...  \n",
       "...                                                  ...  \n",
       "94118  ””We are hoping to seize material evidence to ...  \n",
       "94119  ” On Monday, police secured statements from si...  \n",
       "94120  But the lack of material evidence is making it...  \n",
       "94121         Kim and his son both deny the allegations.  \n",
       "94122  Police are planning to seek arrest warrants fo...  \n",
       "\n",
       "[94123 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.DataFrame([i for i in zip(korean_train_data, english_train_data)], columns=[\"korean\", \"english\"])\n",
    "train_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efbfedf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.sample(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f20822c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data[train_data.duplicated([\"korean\", \"english\"])]))\n",
    "train_data = train_data.drop_duplicates([\"korean\", \"english\"], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98b87b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence, korean=True):\n",
    "    sentence = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s.,?!\\'\\\"]\", \"\", sentence)\n",
    "    sentence = re.sub(\"\\\"+\", \"\", sentence)\n",
    "    sentence = re.sub(r\"\\.+\", \" .\", sentence)\n",
    "    sentence = re.sub(\"!+\", \" !\", sentence)\n",
    "    sentence = re.sub(\"\\?+\", \" ?\", sentence)\n",
    "    sentence.lower().strip()\n",
    "    if not korean:\n",
    "        sentence = \"<sos> \" + sentence + \" <eos>\"\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "226399ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_corpus = train_data[\"korean\"]\n",
    "english_corpus = train_data[\"english\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de05ce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_corpus = [preprocess(sentence, korean=True) for sentence in korean_corpus]\n",
    "english_corpus = [preprocess(sentence, korean=False) for sentence in english_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa8eff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def konlpy_tokenizer(tokenizer, corpus):\n",
    "    words = []\n",
    "    sentences = []\n",
    "    for c in corpus:\n",
    "        temp = tokenizer.morphs(c)\n",
    "        words += temp\n",
    "        sentences.append(temp)\n",
    "        \n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common()\n",
    "    \n",
    "    words = [\"<pad>\", \"<unk>\"] + [key for key, _ in counter]\n",
    "    word_to_index = {word : index for index, word in enumerate(words)}\n",
    "    tensor = []\n",
    "    \n",
    "    for c in sentences:\n",
    "        temp = []\n",
    "        for s in c:\n",
    "            if s in words:\n",
    "                temp.append(word_to_index[s])\n",
    "            else:\n",
    "                temp.append(word_to_index[\"<unk>\"])\n",
    "        tensor.append(temp)\n",
    "    \n",
    "    return tensor, word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "780b4cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = konlpy.tag.Mecab()\n",
    "korean_tokens, korean_word_to_index = konlpy_tokenizer(mecab, korean_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98936fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_SIZE = len(korean_word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a468b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_index_to_word = {word : index for word, index in enumerate(korean_word_to_index)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8874ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=\" \")\n",
    "english_tokenizer.fit_on_texts(english_corpus)\n",
    "english_tokens = english_tokenizer.texts_to_sequences(english_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41094fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = []\n",
    "decoder_input = []\n",
    "\n",
    "for k, e in zip(korean_tokens, english_tokens):\n",
    "    if len(e) < 40 and len(k) < 40:\n",
    "        encoder_input.append(k)\n",
    "        decoder_input.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aeec2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = tf.keras.preprocessing.sequence.pad_sequences(encoder_input, padding='post')\n",
    "decoder_input = tf.keras.preprocessing.sequence.pad_sequences(decoder_input, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e397961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13957\n",
      "13957\n",
      "28771\n",
      "35762\n"
     ]
    }
   ],
   "source": [
    "print(len(encoder_input))\n",
    "print(len(decoder_input))\n",
    "print(WORD_SIZE)\n",
    "print(len(english_tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57163b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i)/d_model)\n",
    "    \n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "    \n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    \n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    \n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82c71512",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "            \n",
    "        self.depth = d_model // self.num_heads\n",
    "            \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "            \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "            \n",
    "    def split_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "        \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "            \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask\n",
    "        )\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "                \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "218abd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92751b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e715d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "       \n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a45eb23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6abd21cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]                  \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cd1e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, \n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 src_vocab_size,\n",
    "                 tgt_vocab_size,\n",
    "                 pos_len,\n",
    "                 dropout=0.2,\n",
    "                 shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9485620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc489da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cceb3975",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(n_layers=2,\n",
    "                          d_model=512,\n",
    "                          n_heads=8,\n",
    "                          d_ff=2048,\n",
    "                          src_vocab_size=WORD_SIZE,\n",
    "                          tgt_vocab_size=len(english_tokenizer.word_index),\n",
    "                          pos_len=39,\n",
    "                          dropout=0.2,\n",
    "                          shared=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b18690c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "801966e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab87b709",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 110/110 [00:43<00:00,  2.55it/s, Loss 1.3574]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 128\n",
    "loss = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, encoder_input.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)  \n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, _, _, _ = train_step(encoder_input[idx:idx+BATCH_SIZE],\n",
    "                                decoder_input[idx:idx+BATCH_SIZE],\n",
    "                                transformer,\n",
    "                                optimizer)\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        loss.append((total_loss.numpy() / (batch + 1)))\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1)) \n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c3ec459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(input_sentence):\n",
    "    sentence = preprocess(input_sentence)\n",
    "    input_sentence = [korean_word_to_index[s] for s in mecab.morphs(input_sentence)]\n",
    "    input_sentence = tf.keras.preprocessing.sequence.pad_sequences([input_sentence], padding=\"post\", maxlen=39)\n",
    "    \n",
    "    output_sentence = []\n",
    "    output_sequence = tf.expand_dims([english_tokenizer.word_index[\"<sos>\"]], 0)\n",
    "    \n",
    "    flag = True\n",
    "    while(flag):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(input_sentence, output_sequence)\n",
    "        predictions, _, _, _ = transformer(input_sentence, output_sequence, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "        \n",
    "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "        \n",
    "        if predicted_id == english_tokenizer.word_index[\"<eos>\"]:\n",
    "            flag = False\n",
    "            \n",
    "        output_sentence.append(english_tokenizer.index_word[predicted_id])\n",
    "        output_sequence = tf.concat([output_sequence, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "    return \" \".join(output_sentence[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24f7827d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it began it began out\n",
      "the war is now war a war .\n",
      "in swine gaza gaza .\n"
     ]
    }
   ],
   "source": [
    "print(decoder_inference(\"처음부터 시작\"))\n",
    "print(decoder_inference(\"전쟁이 발발하였습니다\"))\n",
    "print(decoder_inference(\"지구온난화가 심각하다고 주장했습니다\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b46d82",
   "metadata": {},
   "source": [
    "## 결론\n",
    "### attentional seq2seq와 비교해보기 위해 이전 프로젝트와 전처리를 같게 놓고 모델을 트랜스포머로 바꿔봤습니다. 트랜스포머의 강력함을 엿볼수 있었던 프로젝트ㅋㅋㅋㅋ 좋네..좋아\n",
    "### 일단 이전 attention하고 마찬가지로 데이터를 20000개로 샘플링하고 중복제거 최대 문장길이 날려줬기 때문에 대략 만 4천개 정도를 학습데이터로 잡아서 품질은 그렇게까지 좋다고는 못합니다만, 아무래도 핵심은 어텐션과 트랜스포머와의 비교였기 때문에\n",
    "### 학습 속도부터 비교해 보면 어텐션에서는 학습속도가 매우 느리다는 문제가 있었습니다. 그게 한국어와 영어의 특성때문인지 원인은 알수없지만 트랜스포머를 학습에서부터 굉장히 강력한 퍼포먼스를 보였습니다. 작은 데이터셋에 학습률이 에폭당 너무 빨라서 적당한 에폭에도 오버피팅이 나버린..ㅠ \n",
    "### 데이터의 결과에서도 확연한 차이가 났습니다. 저 문장들이 사실 어텐션에서 너무 번역이 안되서 겨우겨우 비슷한 번역되는 몇개 찾은거랔ㅋㅋㅋㅋ 그거 찾는것도 힘들었는데 확실히 트랜스포머를 왠만한 문장도 다 먹히니 역시 압도적\n",
    "### 끝으로 사실상 이번 프로젝트는 트랜스포머보다 텐서 다루기가 더 늘었던 플젝 하 진짜 텐서 연습해야되는데 너무빡셈 다음플젝까지해서 텐서 잡아놔야지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4b5c7214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e7350c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
